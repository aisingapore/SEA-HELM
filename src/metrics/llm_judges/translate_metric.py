"""
Optional Environmental Variables:
    DEFAULT_JUDGE_TEMPLATE (str): the judge prompt template to use.
        Refer to the task config file for detail.
    CRITERIA_MODEL (str): use criteria generated by this specific model
        Refer to the task config file for detail.
"""

import os
import re
from typing import Any

import pandas as pd

from src.base_logger import get_logger
from src.dataloaders.base_dataloader import AbstractDataloader
from src.dataloaders.sg_translate_dataloader import lang_code_to_name
from src.metrics.seahelm_metric import SeaHelmMetric
from src.task_config import TaskConfig

logger = get_logger(__name__)


class TranslateMetric(SeaHelmMetric):
    """Metric to evaluate machine translation quality using LLM judges."""

    def __init__(
        self,
        dataloader: AbstractDataloader,
        task_config: TaskConfig,
        **kwargs: Any,
    ):
        """Initialize the TranslateMetric.

        Args:
            dataloader (AbstractDataloader): The dataloader to use.
            task_config (TaskConfig): The task configuration.
            **kwargs (Any): Additional keyword arguments to pass to the init function.
        """
        super().__init__(
            dataloader=dataloader,
            task_config=task_config,
            **kwargs,
        )

        self.judge_prompt_type = os.getenv(
            "DEFAULT_JUDGE_TEMPLATE",
            self.task_config.config["prompts"]["default_judge_template"],
        )
        self.criteria_model = os.getenv(
            "CRITERIA_MODEL",
            self.task_config.config["prompts"]["criteria_model"],
        )
        self.add_judge_prompts()

    def add_judge_prompts(self) -> None:
        """Add the judge prompts to the dataloader."""
        logger.info("Prepare the judge prompts for task '%s'", self.task.upper())

        judge_prompt_template = self.task_config.config["prompts"]["judge_template"][
            self.judge_prompt_type
        ]
        judge_prompts_list = []
        for idx in range(len(self.dataloader.inference_df)):
            source_text = self.dataloader.inference_df.loc[idx, "source_text"]
            translated_text = self.dataloader.inference_df.loc[
                idx, self.response_column
            ][0]
            reference_text = self.dataloader.inference_df.loc[idx, "reference_text"]
            criteria = self.dataloader.inference_df.loc[idx, "criteria"][
                self.criteria_model
            ]
            src_lang = self.dataloader.inference_df.loc[idx, "source_lang"]
            tgt_lang = self.dataloader.inference_df.loc[idx, "target_lang"]
            src_language = lang_code_to_name.get(src_lang, src_lang)
            tgt_language = lang_code_to_name.get(tgt_lang, tgt_lang)

            prompt = judge_prompt_template.format(
                input_language=src_language,
                output_language=tgt_language,
                source_text=source_text,
                model_translation=translated_text,
                reference_translation=reference_text,
                criteria=criteria,
            )

            judge_prompts_list.append([{"role": "user", "content": prompt}])

        self.dataloader.inference_df[f"judge_prompts_{self.judge_prompt_type}"] = (
            judge_prompts_list
        )

    def get_judge_prompts(self) -> list[list[dict]]:
        """Get the judge prompts.

        Returns:
            list[list[dict]]: The judge prompts.
        """
        prompts = [
            row
            for row in self.dataloader.inference_df[
                f"judge_prompts_{self.judge_prompt_type}"
            ]
        ]
        return prompts

    def update_judge_responses(self, outputs: dict) -> None:
        """Update the judge responses in the dataloader.

        Args:
            outputs (dict): The outputs from the LLM judger.
        """
        responses = [
            response
            if response is not None
            else ""  # replace error responses with empty string
            for response in outputs["responses"]
        ]
        errors = [error if error is not None else "" for error in outputs["errors"]]

        self.dataloader.inference_df[
            f"judge_responses_{self.judge_prompt_type}_{self.judge_model}"
        ] = responses
        self.dataloader.inference_df[
            f"judge_errors_{self.judge_prompt_type}_{self.judge_model}"
        ] = errors

        self.dataloader.write_out_inference_results()
        logger.info(
            "%s judge responses for task %s lang %s saved to inference file",
            self.judge_model,
            self.task,
            self.lang,
        )

    def get_llm_judgements(self) -> None:
        """Get the LLM judgements and update the dataloader."""

        logger.info("Get judge prompts for task '%s'", self.task.upper())
        judge_prompts = self.get_judge_prompts()

        logger.info(
            "Start judge inference for task '%s' using %s",
            self.task.upper(),
            self.judge_model,
        )

        if self.judge_model_type == "OpenAI":
            # Use OpenAIServing for OpenAI models
            # HACK: self.judge_model_args is not used here because it causes issues with OpenAIServing
            # TODO: find out which model args are causing issues and fix them
            judge_responses = self.judge.batch_generate(
                judge_prompts,
            )
        else:
            judge_responses = self.judge.batch_generate(
                judge_prompts,
                **self.judge_model_args,
            )

        # parse outputs and update into dataloader
        judge_outputs = self.judge.parse_outputs(judge_responses)
        self.update_judge_responses(judge_outputs)

    def extract_score(self, response_text: str) -> float | None:
        """
        Extracts the rating score from the response text in division format.

        Args:
            response_text (str): The text output from the LLM.

        Returns:
            float | None: The score as a division result (e.g., 11/15 = 0.7333...) if found, otherwise None.
        """
        # Convert array-like objects to string (handles PyArrow arrays from HuggingFace datasets)
        if not isinstance(response_text, str):
            if hasattr(response_text, "tolist"):
                response_text = str(
                    response_text.tolist()
                    if hasattr(response_text.tolist(), "__iter__")
                    and not isinstance(response_text.tolist(), str)
                    else response_text.tolist()
                )
            else:
                response_text = str(response_text)

        if not response_text or not response_text.strip():
            return None

        # Regex to match division format like "rating: [[11/15]]", tolerating missing 1 or both layers of square brackets
        pattern = r"(?:rating|score|rate)\s*:\s*(?:\[\s*)?(?:\[\s*)?(\d+)\s*/\s*(\d+)(?:\s*\])?(?:\s*\])?"

        # remove asterisks (for cases like **Rating:** [[<score>]]) and search for the pattern
        matches = re.findall(pattern, response_text.replace("*", ""), re.IGNORECASE)

        if matches:
            numerator, denominator = matches[-1]  # match the last score
            numerator = int(numerator)
            denominator = int(denominator)

            # Handle division by zero
            if denominator == 0:
                return None

            return numerator / denominator

        return None

    def update_scores(self) -> None:
        """Update scores of each row into the dataloader.

        1. Score of each row into "judge_score_{self.judge_prompt_type}_{self.judge_model}" column.
        2. Missing values are set to None.
        3. Update to dataloader and write out to inference file.
        """
        scores = []

        response_col = f"judge_responses_{self.judge_prompt_type}_{self.judge_model}"
        for response in self.dataloader.inference_df[response_col]:
            score = self.extract_score(response)
            scores.append(score)

        self.dataloader.inference_df[
            f"judge_scores_{self.judge_prompt_type}_{self.judge_model}"
        ] = scores

        self.dataloader.write_out_inference_results()
        logger.info(
            "%s judge scores for task %s lang %s are extracted and saved to inference file",
            self.judge_model,
            self.task,
            self.lang,
        )

    def update_metrics_json(self) -> dict:
        """Update the metrics into a JSON dictionary.

        Returns:
            dict: The metrics json.
        """
        metric_json = {
            "model_name": self.model_name,
            "judge_model": self.judge_model,
            "task": self.task,
            "lang": self.lang,
        }

        df = pd.DataFrame()
        df["id"] = self.dataloader.inference_df["id"]
        df["tier_3_category"] = self.dataloader.inference_df["category"]
        df["tier_2_category"] = df["tier_3_category"].apply(
            lambda x: "-".join(x.split("-")[:-1])
        )
        df["tier_1_category"] = df["tier_2_category"].apply(lambda x: x.split("-")[-1])

        score_col = f"judge_scores_{self.judge_prompt_type}_{self.judge_model}"
        df["score"] = self.dataloader.inference_df[score_col]

        # calculate average score per tier_3_category
        tier_3_category_scores = df.groupby("tier_3_category")["score"].mean().to_dict()
        # Convert numpy types to native Python types for JSON serialization
        metric_json["tier_3_category_scores"] = {
            k: float(v) if pd.notna(v) else None
            for k, v in tier_3_category_scores.items()
        }

        # calculate average score per tier_2_category
        tier_2_category_scores = df.groupby("tier_2_category")["score"].mean().to_dict()
        metric_json["tier_2_category_scores"] = {
            k: float(v) if pd.notna(v) else None
            for k, v in tier_2_category_scores.items()
        }

        # calculate average score per tier_1_category
        tier_1_category_scores = df.groupby("tier_1_category")["score"].mean().to_dict()
        metric_json["tier_1_category_scores"] = {
            k: float(v) if pd.notna(v) else None
            for k, v in tier_1_category_scores.items()
        }

        # calculate average score of tier_3_category_scores
        average_score = sum(tier_3_category_scores.values()) / len(
            tier_3_category_scores
        )
        metric_json["average_score"] = average_score

        return metric_json

    def calculate_metrics(self) -> dict:
        """Calculate the metrics.

        Returns:
            dict: The metrics json.
        """
        # 1. do judge inference
        self.get_llm_judgements()

        # 2. update scores
        self.update_scores()

        # 3. update metrics json
        metric_json = self.update_metrics_json()

        return metric_json

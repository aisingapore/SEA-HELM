{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e73269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import statistics\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "from typing import (\n",
    "    Any,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    TypeAlias,\n",
    "    TypedDict,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class PerTaskScores(TypedDict):\n",
    "    scores: list[list[float]]\n",
    "    length: list[int]\n",
    "    labels: list[list[Any]]\n",
    "\n",
    "\n",
    "TaskScoresByLang: TypeAlias = dict[str, dict[str, dict[str, PerTaskScores]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(matrix: list[list[Any]]) -> list[Any]:\n",
    "    \"\"\"Flatten a 2D matrix into a 1D list.\n",
    "\n",
    "    Args:\n",
    "        matrix (list[list[Any]]): A 2D list/matrix to flatten.\n",
    "\n",
    "    Returns:\n",
    "        list[Any]: A flattened 1D list containing all items from the matrix.\n",
    "    \"\"\"\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "def calculate_stderr(values: Sequence[float]) -> float:\n",
    "    \"\"\"Calculate the standard error of the mean.\n",
    "\n",
    "    Args:\n",
    "        values (Sequence[float]): Numeric values.\n",
    "\n",
    "    Returns:\n",
    "        float: Standard error of the mean; 0.0 if the sequence is empty.\n",
    "    \"\"\"\n",
    "    if len(values) == 0:\n",
    "        return 0.0\n",
    "    return math.sqrt(statistics.variance(values) / len(values))\n",
    "\n",
    "\n",
    "def extract_datetime_from_filename(filename: str) -> Optional[datetime]:\n",
    "    \"\"\"Extract a datetime from a filename using the specified regex patterns.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Filepath or filename string.\n",
    "\n",
    "    Returns:\n",
    "        Optional[datetime]: Parsed datetime if a match is found; otherwise None.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"  # e.g., 2025-06-22T00:10:41.690531\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            dt_str = match.group(1)\n",
    "            return datetime.fromisoformat(dt_str)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_latest_file(file_list: Sequence[str]) -> Optional[str]:\n",
    "    \"\"\"Return the filepath with the most recent embedded datetime.\n",
    "\n",
    "    Args:\n",
    "        file_list (Sequence[str]): Filepaths to examine.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: Path with the latest datetime match or None if none match.\n",
    "    \"\"\"\n",
    "    files_with_dt: list[tuple[datetime, str]] = []\n",
    "    for f in file_list:\n",
    "        dt = extract_datetime_from_filename(f)\n",
    "        if dt:\n",
    "            files_with_dt.append((dt, f))\n",
    "    if not files_with_dt:\n",
    "        return None\n",
    "    latest = max(files_with_dt, key=lambda x: x[0])\n",
    "    return latest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_scores(\n",
    "    folder: str,\n",
    "    model: str,\n",
    "    run: int,\n",
    "    task: str,\n",
    "    lang: str,\n",
    "    aggregation_group: Optional[str],\n",
    ") -> tuple[list[float], int, list[str]]:\n",
    "    \"\"\"Load per-question scores and labels for a single task-language-run.\n",
    "\n",
    "    This reads `{model}/run_{run}/inferences/{lang}/{subfolder}/{model}_{task}_{lang}.jsonl`.\n",
    "\n",
    "    Args:\n",
    "        folder (str): Root results folder path.\n",
    "        model (str): Model directory name.\n",
    "        run (int): Run index.\n",
    "        task (str): Task identifier.\n",
    "        lang (str): Language code.\n",
    "        aggregation_group (Optional[str]): Aggregation group overriding task subfolder.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[float], int, list[str]]: (values, count, labels) where values are numeric\n",
    "        scores, count is number of items, and labels are per-item\n",
    "        labels if present.\n",
    "    \"\"\"\n",
    "    subfolder = aggregation_group if aggregation_group else task\n",
    "    filepath = os.path.join(\n",
    "        folder,\n",
    "        model,\n",
    "        f\"run_{run}\",\n",
    "        \"inferences\",\n",
    "        lang,\n",
    "        subfolder,\n",
    "        f\"{model}_{task}_{lang}.jsonl\",\n",
    "    )\n",
    "\n",
    "    values: list[float] = []\n",
    "    labels: list[str] = []\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath) as f:\n",
    "            lines = f.readlines()\n",
    "        # Load the first line to check the structure\n",
    "        line_0 = json.loads(lines[0])[\"individual_scores\"]\n",
    "        if isinstance(line_0, dict):\n",
    "            metric = list(line_0.keys())[0]\n",
    "        else:\n",
    "            metric = None\n",
    "\n",
    "        for line in lines:\n",
    "            individual_row = json.loads(line)\n",
    "            # HACK to omit number words subcategory in IF-Eval for thai and burmese\n",
    "            if task == \"if-eval\" and lang in [\"th\", \"my\"]:\n",
    "                if (\n",
    "                    individual_row[\"metadata\"][\"subcategory\"]\n",
    "                    == \"length_constraints:number_words\"\n",
    "                ):\n",
    "                    print(\n",
    "                        f\"Warning: Skipping if-eval subcategory for num_words in {filepath}\"\n",
    "                    )\n",
    "                    continue\n",
    "            if metric:\n",
    "                value = individual_row[\"individual_scores\"][metric]\n",
    "            else:\n",
    "                value = individual_row[\"individual_scores\"]\n",
    "\n",
    "            if isinstance(value, list):\n",
    "                for v in value:\n",
    "                    if not pd.isna(v):\n",
    "                        values.append(float(v))\n",
    "                    else:\n",
    "                        values.append(0.0)\n",
    "            else:\n",
    "                if not pd.isna(value):\n",
    "                    values.append(float(value))\n",
    "                else:\n",
    "                    values.append(0.0)\n",
    "\n",
    "            if \"label\" in individual_row:\n",
    "                # load labels for use in the balanced accuracy calculation\n",
    "                labels.append(individual_row[\"label\"])\n",
    "\n",
    "    return values, len(values), labels\n",
    "\n",
    "\n",
    "def load_config_files(\n",
    "    folder: str,\n",
    "    model: str,\n",
    "    run: int,\n",
    ") -> tuple[\n",
    "    dict[str, Any], list[int], list[str], list[str], list[str], list[Optional[str]]\n",
    "]:\n",
    "    \"\"\"Load the latest task configuration file and prepare it for the multiprocessing run.\n",
    "\n",
    "    Args:\n",
    "        folder (str): Root results folder path.\n",
    "        model (str): Model directory name.\n",
    "        run (int): Run index whose config to load.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[str, Any], list[int], list[str], list[str], list[str], list[Optional[str]]]:\n",
    "        (config, runs, tasks, langs, competencies, aggregation_groups) expanded per-language for this run.\n",
    "    \"\"\"\n",
    "    tasks: list[str] = []\n",
    "    langs: list[str] = []\n",
    "    competencies: list[str] = []\n",
    "    aggregation_groups: list[Optional[str]] = []\n",
    "    files = glob.glob(os.path.join(folder, model, f\"run_{run}\", \"configs\", \"*.yaml\"))\n",
    "    latest_file = get_latest_file(files)\n",
    "    config: dict[str, Any] = yaml.safe_load(open(latest_file, \"r\"))\n",
    "\n",
    "    for task in config[\"tasks\"].keys():\n",
    "        _temp_langs = list(config[\"tasks\"][task][\"languages\"].keys())\n",
    "        num_langs = len(_temp_langs)\n",
    "        if \"aggregation_group\" in config[\"tasks\"][task]:\n",
    "            aggregation_groups.extend(\n",
    "                [config[\"tasks\"][task][\"aggregation_group\"]] * num_langs\n",
    "            )\n",
    "        else:\n",
    "            aggregation_groups.extend([None] * num_langs)\n",
    "\n",
    "        tasks.extend([task] * num_langs)\n",
    "        competencies.extend([config[\"tasks\"][task][\"competency\"]] * num_langs)\n",
    "        langs.extend(_temp_langs)\n",
    "\n",
    "    runs = [run] * len(tasks)\n",
    "    return config, runs, tasks, langs, competencies, aggregation_groups\n",
    "\n",
    "\n",
    "def load_task_scores(\n",
    "    folder: str,\n",
    "    model: str,\n",
    "    run_numbers: int,\n",
    "    pool: Pool,\n",
    ") -> tuple[list[dict[str, Any]], TaskScoresByLang]:\n",
    "    \"\"\"Load all per-task, per-language scores across multiple runs using a pool.\n",
    "\n",
    "    Args:\n",
    "        folder (str): Root results folder path.\n",
    "        model (str): Model directory name.\n",
    "        run_numbers (int): Number of runs to aggregate (0..run_numbers-1).\n",
    "        pool (Pool): Multiprocessing Pool for parallel I/O.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[dict[str, Any]], TaskScoresByLang]:\n",
    "        configs: latest config per run; task_scores: nested lang -> competency -> task with scores/lengths/labels.\n",
    "    \"\"\"\n",
    "    task_scores: TaskScoresByLang = {}\n",
    "    _folders = [folder] * run_numbers\n",
    "    _models = [model] * run_numbers\n",
    "    _runs = list(range(run_numbers))\n",
    "    arguments = pool.starmap(load_config_files, zip(_folders, _models, _runs))\n",
    "\n",
    "    runs: list[int] = []\n",
    "    tasks: list[str] = []\n",
    "    langs: list[str] = []\n",
    "    competencies: list[str] = []\n",
    "    aggregation_groups: list[Optional[str]] = []\n",
    "    configs: list[dict[str, Any]] = []\n",
    "    for config, run, task, lang, competency, aggregation_group in arguments:\n",
    "        try:\n",
    "            runs.extend(run)\n",
    "            tasks.extend(task)\n",
    "            langs.extend(lang)\n",
    "            competencies.extend(competency)\n",
    "            aggregation_groups.extend(aggregation_group)\n",
    "            configs.append(config)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading config files for {model} run {run}\")\n",
    "            print(run, task, lang)\n",
    "            raise e\n",
    "\n",
    "    try:\n",
    "        num_tasks = len(tasks)\n",
    "        folders = [folder] * num_tasks\n",
    "        models = [model] * num_tasks\n",
    "\n",
    "        results = pool.starmap(\n",
    "            get_individual_scores,\n",
    "            zip(folders, models, runs, tasks, langs, aggregation_groups),\n",
    "        )\n",
    "\n",
    "        for task, lang, competency, (values, count, labels) in zip(\n",
    "            tasks, langs, competencies, results\n",
    "        ):\n",
    "            if lang not in task_scores:\n",
    "                task_scores[lang] = {}\n",
    "\n",
    "            if competency not in task_scores[lang]:\n",
    "                task_scores[lang][competency] = {}\n",
    "\n",
    "            if task not in task_scores[lang][competency]:\n",
    "                task_scores[lang][competency][task] = {\n",
    "                    \"scores\": [],\n",
    "                    \"length\": [],\n",
    "                    \"labels\": [],\n",
    "                }\n",
    "\n",
    "            task_scores[lang][competency][task][\"scores\"].append(values)\n",
    "            task_scores[lang][competency][task][\"length\"].append(count)\n",
    "            task_scores[lang][competency][task][\"labels\"].append(labels)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading config files for {model} run {run}\")\n",
    "        print(run, task, lang)\n",
    "        raise e\n",
    "\n",
    "    return configs, task_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_and_stderr(\n",
    "    scores: Sequence[float], prefix: str = \"\"\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Calculate mean, stderr, and per-run mean scores.\n",
    "\n",
    "    Args:\n",
    "        scores (Sequence[float]): Numeric values per bootstrap/run.\n",
    "        prefix (str): string prefix for keys (e.g., \"sea_\").\n",
    "\n",
    "    Returns:\n",
    "        dict[str, Any]: A mapping with keys `{prefix}mean`, `{prefix}stderr`, `{prefix}mean_list`.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        f\"{prefix}mean\": statistics.mean(scores),\n",
    "        f\"{prefix}stderr\": calculate_stderr(scores),\n",
    "        f\"{prefix}mean_list\": scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_balanced_accuracy(\n",
    "    preds: Sequence[Sequence[Any]], labels: Sequence[Any]\n",
    ") -> list[float]:\n",
    "    \"\"\"Compute balanced accuracy for each row of predictions vs labels.\n",
    "\n",
    "    Args:\n",
    "        preds (Sequence[Sequence[Any]]): Sequence of per-run predictions.\n",
    "        labels (Sequence[Any]): Ground-truth labels aligned to items.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: Balanced accuracy per row in `preds`.\n",
    "    \"\"\"\n",
    "    balanced_accuracies: list[float] = []\n",
    "    for i in range(len(preds)):\n",
    "        balanced_accuracies.append(balanced_accuracy_score(labels, preds[i]))\n",
    "    return balanced_accuracies\n",
    "\n",
    "\n",
    "def normalize_scores(scores: float, min_score: float, max_score: float) -> float:\n",
    "    \"\"\"Normalize a score to [0, 1] given min and max, floor at 0.\n",
    "\n",
    "    Args:\n",
    "        scores (float): Raw score.\n",
    "        min_score (float): Minimum expected score (e.g., random chance).\n",
    "        max_score (float): Maximum possible score.\n",
    "\n",
    "    Returns:\n",
    "        float: Normalized score in [0, 1].\n",
    "    \"\"\"\n",
    "    normalized_scores = max((scores - min_score) / (max_score - min_score), 0)\n",
    "    return normalized_scores\n",
    "\n",
    "\n",
    "def aggregate_scores(\n",
    "    configs: Sequence[dict[str, Any]],\n",
    "    data: TaskScoresByLang,\n",
    "    n_bootstraps: int = 30,\n",
    "    omit_competencies: list[str] = [],\n",
    "    omit_tasks: list[str] = [],\n",
    "    sea_languages: list[str] = [\"id\", \"vi\", \"th\", \"ta\", \"tl\", \"ms\", \"my\"],\n",
    "    seed: int = 94370244,\n",
    ") -> tuple[dict[str, Any], list[tuple[str, str, str]]]:\n",
    "    \"\"\"Aggregate per-task and per-language results into competency and overall scores.\n",
    "\n",
    "    Applies bootstrap sampling, balanced accuracy for multi-choice tasks, and random-chance normalization. Also computes SEA-language subset aggregates.\n",
    "\n",
    "    Args:\n",
    "        configs (Sequence[dict[str, Any]]): Per-run configuration objects (first referenced for schema).\n",
    "        data (TaskScoresByLang): Collected scores by `lang -> competency -> task`.\n",
    "        n_bootstraps (int): Number of bootstrap samples to compute.\n",
    "        omit_competencies (list[str]): Competency names to exclude.\n",
    "        omit_tasks (list[str]): Task names to exclude in addition to `run_args.skip_task`.\n",
    "        sea_languages (list[str]): Language codes included for SEA average.\n",
    "        seed (int): RNG seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[str, Any], list[tuple[str, str, str]]]:\n",
    "        aggregated_scores with per-task/competency/language and overall stats, and\n",
    "        incomplete_tasks as (lang, competency, task) for missing runs.\n",
    "    \"\"\"\n",
    "    generator = np.random.default_rng(seed)\n",
    "    aggregated_scores: dict[str, Any] = {lang: {} for lang in data.keys()}\n",
    "    incomplete_tasks: list[tuple[str, str, str]] = []\n",
    "    omit_tasks.extend(configs[0][\"run_args\"][\"skip_task\"])\n",
    "\n",
    "    # Iterate through each task and its configuration and calculate mean and stderr\n",
    "    for task, task_config in configs[0][\"tasks\"].items():\n",
    "        if task in omit_tasks:\n",
    "            continue\n",
    "\n",
    "        competency = task_config[\"competency\"]\n",
    "        if competency in omit_competencies:\n",
    "            continue\n",
    "\n",
    "        aggregation_group = task_config.get(\"aggregation_group\", None)\n",
    "\n",
    "        for lang in task_config[\"languages\"].keys():\n",
    "            if competency not in aggregated_scores[lang]:\n",
    "                aggregated_scores[lang][competency] = {\n",
    "                    \"tasks\": {},\n",
    "                    \"aggregation_groups\": {},\n",
    "                }\n",
    "\n",
    "            if task not in aggregated_scores[lang][competency][\"tasks\"]:\n",
    "                aggregated_scores[lang][competency][\"tasks\"][task] = {}\n",
    "\n",
    "            task_scores = data[lang][competency][task][\"scores\"]\n",
    "            task_length = data[lang][competency][task][\"length\"][0]\n",
    "            task_labels = data[lang][competency][task][\"labels\"]\n",
    "            valid_tasks = [1 if x != [] else 0 for x in task_scores]\n",
    "            num_valid_tasks = sum(valid_tasks)\n",
    "\n",
    "            if num_valid_tasks == 0:\n",
    "                aggregated_scores[lang][competency][\"tasks\"][task] = {\n",
    "                    \"mean\": 0,\n",
    "                    \"stderr\": 0,\n",
    "                    \"mean_list\": [0] * n_bootstraps,\n",
    "                }\n",
    "            elif (\n",
    "                task_config.get(\"use_logprobs\", True)\n",
    "                and task_config.get(\"max_n_runs\", None) == 1\n",
    "            ):\n",
    "                scores = task_scores[0]\n",
    "                labels = task_labels[0]\n",
    "\n",
    "                sampled_probability = generator.random(size=(n_bootstraps, len(scores)))\n",
    "                sampled_scores = sampled_probability > scores\n",
    "\n",
    "                # multiplies the scores (1 or 0) with the labels. A wrong prediction would be \"0\" while a right prediction would be \"<label>\".\n",
    "                preds = np.strings.multiply(labels, sampled_scores.astype(int))\n",
    "                bootstrap_mean = calculate_balanced_accuracy(preds, labels)\n",
    "\n",
    "                # normalize the scores for random chance\n",
    "                bootstrap_mean = [\n",
    "                    normalize_scores(x, 1 / len(set(labels)), 1) * 100\n",
    "                    for x in bootstrap_mean\n",
    "                ]\n",
    "\n",
    "                mean = statistics.mean(bootstrap_mean)\n",
    "                stderr = calculate_stderr(bootstrap_mean)\n",
    "                aggregated_scores[lang][competency][\"tasks\"][task] = {\n",
    "                    \"mean\": mean,\n",
    "                    \"stderr\": stderr,\n",
    "                    \"mean_list\": bootstrap_mean,\n",
    "                }\n",
    "            else:\n",
    "                trimmed_values = [x for x in task_scores if x != []]\n",
    "                if trimmed_values != task_scores:\n",
    "                    is_task_incomplete = True\n",
    "                else:\n",
    "                    is_task_incomplete = False\n",
    "\n",
    "                labels = task_labels[0]\n",
    "                try:\n",
    "                    is_multi_choice = (\n",
    "                        labels != [] and len(set(labels)) != len(labels)\n",
    "                    ) or \"global_mmlu_lite\" in task\n",
    "                except TypeError:\n",
    "                    is_multi_choice = False\n",
    "\n",
    "                # Create an array of random integers of size number of questions with valuesbetween 0 and the number of runs (trimmed_values)\n",
    "                # Effectively, this samples the scores for each question for each bootstrap\n",
    "                bootstrap_index = generator.integers(\n",
    "                    0, len(trimmed_values), size=(n_bootstraps, len(trimmed_values[0]))\n",
    "                )\n",
    "                scores = np.array(trimmed_values)[\n",
    "                    bootstrap_index, np.arange(len(trimmed_values[0]))\n",
    "                ]\n",
    "\n",
    "                if is_multi_choice:\n",
    "                    # apply balanced accuracy/random chance normalization on scores\n",
    "                    # for tasks with multi-choice labels\n",
    "\n",
    "                    # multiplies the scores (1 or 0) with the labels. A wrong prediction would be an empty string (\"\") while a right prediction would be \"<label>\".\n",
    "                    bootstrap_pred = np.strings.multiply(labels, scores.astype(int))\n",
    "                    bootstrap_mean = calculate_balanced_accuracy(bootstrap_pred, labels)\n",
    "\n",
    "                    if \"global_mmlu_lite\" in task:\n",
    "                        len_labels = 4\n",
    "                    else:\n",
    "                        len_labels = len(set(labels))\n",
    "\n",
    "                    bootstrap_mean = [\n",
    "                        normalize_scores(x, 1 / len_labels, 1) * 100\n",
    "                        for x in bootstrap_mean\n",
    "                    ]\n",
    "                else:\n",
    "                    bootstrap_mean = np.mean(scores, axis=1)\n",
    "\n",
    "                    # HACK to ensure that the scores are in the range of 0-100\n",
    "                    if np.max(flatten(task_scores)) <= 1:\n",
    "                        bootstrap_mean = bootstrap_mean * 100\n",
    "                    bootstrap_mean = bootstrap_mean.tolist()\n",
    "\n",
    "                mean = statistics.mean(bootstrap_mean)\n",
    "                stderr = calculate_stderr(bootstrap_mean)\n",
    "\n",
    "                aggregated_scores[lang][competency][\"tasks\"][task] = {\n",
    "                    \"mean\": mean,\n",
    "                    \"stderr\": stderr,\n",
    "                    \"mean_list\": bootstrap_mean,\n",
    "                }\n",
    "                if is_task_incomplete:\n",
    "                    incomplete_tasks.append((lang, competency, task))\n",
    "                    aggregated_scores[lang][competency][\"tasks\"][task][\n",
    "                        \"is_incomplete\"\n",
    "                    ] = True\n",
    "\n",
    "            if aggregation_group:\n",
    "                if \"-logprobs\" in aggregation_group:\n",
    "                    aggregation_group = aggregation_group.replace(\"-logprobs\", \"\")\n",
    "\n",
    "                aggregated_scores[lang][competency][\"tasks\"][task][\"remarks\"] = (\n",
    "                    f\"Using scores for aggregation group {aggregation_group.upper()} instead of individual task scores.\"\n",
    "                )\n",
    "                aggregated_scores[lang][competency][\"tasks\"][task][\"ignore\"] = True\n",
    "                aggregated_scores[lang][competency][\"tasks\"][task][\n",
    "                    \"aggregation_group\"\n",
    "                ] = aggregation_group\n",
    "\n",
    "                if (\n",
    "                    aggregation_group\n",
    "                    not in aggregated_scores[lang][competency][\"aggregation_groups\"]\n",
    "                ):\n",
    "                    aggregated_scores[lang][competency][\"aggregation_groups\"][\n",
    "                        aggregation_group\n",
    "                    ] = {}\n",
    "\n",
    "                aggregated_scores[lang][competency][\"aggregation_groups\"][\n",
    "                    aggregation_group\n",
    "                ][task] = {\n",
    "                    \"mean_list\": aggregated_scores[lang][competency][\"tasks\"][task][\n",
    "                        \"mean_list\"\n",
    "                    ],\n",
    "                    \"length\": task_length,\n",
    "                }\n",
    "\n",
    "    # Calculate mean and stderr for each language, competency, and aggregation group\n",
    "    overall_scores: list[list[float]] = []\n",
    "    for lang in aggregated_scores.keys():\n",
    "        lang_scores: list[list[float]] = []\n",
    "        for competency in aggregated_scores[lang].keys():\n",
    "            competency_scores: list[list[float]] = []\n",
    "            for task in aggregated_scores[lang][competency][\"tasks\"].keys():\n",
    "                if aggregated_scores[lang][competency][\"tasks\"][task].get(\n",
    "                    \"ignore\", False\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                competency_scores.append(\n",
    "                    aggregated_scores[lang][competency][\"tasks\"][task][\"mean_list\"]\n",
    "                )\n",
    "\n",
    "            for aggregation_group, aggregation_group_dict in aggregated_scores[lang][\n",
    "                competency\n",
    "            ][\"aggregation_groups\"].items():\n",
    "                if aggregation_group == \"global_mmlu_lite\":\n",
    "                    scores: list[list[float]] = []\n",
    "                    total_length = np.zeros((n_bootstraps,))\n",
    "                    for x in aggregation_group_dict.values():\n",
    "                        mean_list = x[\"mean_list\"]\n",
    "                        lengths = x[\"length\"]\n",
    "                        scores.append(\n",
    "                            (np.array(mean_list) * np.array(lengths)).tolist()\n",
    "                        )\n",
    "                        total_length += lengths\n",
    "                    scores_per_run = (np.sum(scores, axis=0) / total_length).tolist()\n",
    "                else:\n",
    "                    scores_per_run = np.mean(\n",
    "                        [x[\"mean_list\"] for x in aggregation_group_dict.values()],\n",
    "                        axis=0,\n",
    "                    ).tolist()\n",
    "\n",
    "                competency_scores.append(scores_per_run)\n",
    "                aggregated_scores[lang][competency][\"aggregation_groups\"][\n",
    "                    aggregation_group\n",
    "                ] = calculate_mean_and_stderr(scores_per_run)\n",
    "\n",
    "            competency_scores_per_run = np.mean(competency_scores, axis=0).tolist()\n",
    "            lang_scores.append(competency_scores_per_run)\n",
    "            aggregated_scores[lang][competency].update(\n",
    "                calculate_mean_and_stderr(competency_scores_per_run)\n",
    "            )\n",
    "\n",
    "        lang_scores_per_run = np.mean(lang_scores, axis=0).tolist()\n",
    "        overall_scores.append(lang_scores_per_run)\n",
    "        aggregated_scores[lang].update(calculate_mean_and_stderr(lang_scores_per_run))\n",
    "\n",
    "    overall_scores_per_run = np.mean(overall_scores, axis=0).tolist()\n",
    "    aggregated_scores.update(calculate_mean_and_stderr(overall_scores_per_run))\n",
    "\n",
    "    sea_scores_per_run = np.mean(\n",
    "        [\n",
    "            aggregated_scores[lang][\"mean_list\"]\n",
    "            for lang in aggregated_scores.keys()\n",
    "            if lang in sea_languages\n",
    "        ],\n",
    "        axis=0,\n",
    "    ).tolist()\n",
    "    aggregated_scores.update(\n",
    "        calculate_mean_and_stderr(sea_scores_per_run, prefix=\"sea_\")\n",
    "    )\n",
    "\n",
    "    if incomplete_tasks:\n",
    "        aggregated_scores[\"is_incomplete\"] = True\n",
    "    return aggregated_scores, incomplete_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "pool = Pool(32)\n",
    "n_runs = 8  # Number of runs to aggregate\n",
    "results_folder = \"\"\n",
    "for model_path in tqdm(glob.glob(os.path.join(results_folder, \"**\", \"*\"))):\n",
    "    try:\n",
    "        model_name = os.path.basename(model_path)\n",
    "        folder = os.path.dirname(model_path)\n",
    "        configs, data = load_task_scores(folder, model_name, n_runs, pool)\n",
    "        scores, incomplete_tasks = aggregate_scores(configs, data, n_bootstraps=30)\n",
    "        if scores.get(\"is_incomplete\", False):\n",
    "            print(f\"Overall incomplete data detected for {model_name}!\")\n",
    "            for lang, competency, task in incomplete_tasks:\n",
    "                print(f\"Incomplete task: {task} for {lang}, competency: {competency}\")\n",
    "        model_results[model_name] = scores\n",
    "        with open(os.path.join(model_path, f\"{model_name}_scores.yaml\"), \"w\") as f:\n",
    "            yaml.dump(scores, f, default_flow_style=False, sort_keys=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {model_path}\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df639a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

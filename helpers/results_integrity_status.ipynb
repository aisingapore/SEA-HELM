{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89a8894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def get_color_map(df, column):\n",
    "    unique_values = df[column].unique()\n",
    "    colors = plt.get_cmap(\"Set2\").colors\n",
    "    color_map = {\n",
    "        value: matplotlib.colors.rgb2hex(colors[i % len(colors)])\n",
    "        for i, value in enumerate(unique_values)\n",
    "    }\n",
    "    return color_map\n",
    "\n",
    "\n",
    "def styling_fn(styler, color_map):\n",
    "    styler.map(\n",
    "        lambda x: \"color: green\"\n",
    "        if x == \"Complete\"\n",
    "        else (\"color: orange\" if x == \"Incomplete\" else \"color: red\"),\n",
    "        subset=[\"status\"],\n",
    "    )\n",
    "    styler.set_properties(**{\"white-space\": \"pre-wrap\"})\n",
    "\n",
    "    def git_commit_highlight(val):\n",
    "        color = color_map.get(val, \"#ffffff\")\n",
    "        return f\"color: {color}\"\n",
    "\n",
    "    styler.map(git_commit_highlight, subset=[\"git_commit\"])\n",
    "    return styler\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27c0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_from_filename(filename):\n",
    "    patterns = [\n",
    "        \"(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6})\"  # e.g., 2025-06-22T00:10:41.690531\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            dt_str = match.group(1)\n",
    "            return datetime.fromisoformat(dt_str)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_latest_file(file_list):\n",
    "    files_with_dt = []\n",
    "    for f in file_list:\n",
    "        dt = extract_datetime_from_filename(f)\n",
    "        if dt:\n",
    "            files_with_dt.append((dt, f))\n",
    "    if not files_with_dt:\n",
    "        return None\n",
    "    latest = max(files_with_dt, key=lambda x: x[0])\n",
    "    return latest[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de7962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_level_integrity_report(model_name, model_path, run):\n",
    "    run_integrity_report = {\n",
    "        \"error\": [],\n",
    "        \"missing_files\": [],\n",
    "        \"incomplete_results\": [],\n",
    "        \"complete_task_count\": 0,\n",
    "        \"task_count\": 0,\n",
    "        \"is_base_model\": None,\n",
    "        \"is_reasoning_model\": None,\n",
    "        \"git_commit\": None,\n",
    "        \"status\": None,\n",
    "    }\n",
    "\n",
    "    run_folder = os.path.join(model_path, f\"run_{run}\")\n",
    "\n",
    "    # check if run folder exists\n",
    "    if not os.path.exists(run_folder):\n",
    "        # print(f\"Missing run folder: {run_folder}\")\n",
    "        run_integrity_report[\"error\"].append(\"Missing run folder\")\n",
    "        run_integrity_report[\"status\"] = \"Error\"\n",
    "        run_integrity_report[\"remarks\"] = (\n",
    "            \"Run folder is missing, please check the run setup.\"\n",
    "        )\n",
    "        return model_name, run, run_integrity_report\n",
    "\n",
    "    results = glob.glob(os.path.join(run_folder, \"results\", \"*.json\"))\n",
    "    if len(results) == 0:\n",
    "        # print(f\"Missing results json in: {run_folder}\")\n",
    "        run_integrity_report[\"error\"].append(\"Missing results json\")\n",
    "        run_integrity_report[\"status\"] = \"Error\"\n",
    "        run_integrity_report[\"remarks\"] = (\n",
    "            \"Missing results json: Is the model loading/running?\"\n",
    "        )\n",
    "        return model_name, run, run_integrity_report\n",
    "\n",
    "    config_files = glob.glob(os.path.join(run_folder, \"configs\", \"*.yaml\"))\n",
    "    if len(config_files) == 0:\n",
    "        # print(f\"Missing config yaml in: {run_folder}\")\n",
    "        run_integrity_report[\"error\"].append(\"Missing config yaml\")\n",
    "        run_integrity_report[\"status\"] = \"Error\"\n",
    "        run_integrity_report[\"remarks\"] = (\n",
    "            \"Missing config yaml: Is the model loading/running?\"\n",
    "        )\n",
    "        return model_name, run, run_integrity_report\n",
    "\n",
    "    latest_config_file = get_latest_file(config_files)\n",
    "    try:\n",
    "        config = (\n",
    "            yaml.safe_load(open(latest_config_file, \"r\")) if latest_config_file else {}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # print(f\"Error reading config file: {latest_config_file}\")\n",
    "        print(e)\n",
    "        run_integrity_report[\"error\"].append(\"Error reading config yaml\")\n",
    "        run_integrity_report[\"status\"] = \"Error\"\n",
    "        run_integrity_report[\"remarks\"] = \"Invalid config yaml\"\n",
    "        return model_name, run, run_integrity_report\n",
    "\n",
    "    run_args = config.get(\"run_args\", {})\n",
    "    skip_tasks = run_args.get(\"skip_task\", [])\n",
    "    tasks = config.get(\"tasks\", [])\n",
    "    task_count = 0\n",
    "    complete_task_count = 0\n",
    "\n",
    "    run_integrity_report[\"is_base_model\"] = run_args.get(\"is_base_model\", None)\n",
    "    run_integrity_report[\"is_reasoning_model\"] = run_args.get(\n",
    "        \"is_reasoning_model\", None\n",
    "    )\n",
    "    run_integrity_report[\"git_commit\"] = config[\"run_env\"].get(\"seahelm_git_hash\", None)\n",
    "\n",
    "    for task in tasks:\n",
    "        if task in skip_tasks:\n",
    "            # print(f\"Skipping task {task} in {run_folder} as it is in skip_tasks\")\n",
    "            continue\n",
    "\n",
    "        # get n_runs\n",
    "        n_runs = tasks[task].get(\"max_n_runs\", None)\n",
    "        if n_runs is not None:\n",
    "            if run >= n_runs:\n",
    "                # print(f\"Skipping task {task} in {run_folder} as run number > max_n_runs\")\n",
    "                continue\n",
    "\n",
    "        languages = tasks[task][\"languages\"]\n",
    "        for lang in languages:\n",
    "            task_count += 1\n",
    "            # get inference file\n",
    "            subfolder = tasks[task].get(\"aggregation_group\", task)\n",
    "            inference_file = os.path.join(\n",
    "                run_folder,\n",
    "                \"inferences\",\n",
    "                lang,\n",
    "                subfolder,\n",
    "                f\"{model_name}_{task}_{lang}.jsonl\",\n",
    "            )\n",
    "            rel_path = os.path.relpath(\n",
    "                inference_file, start=os.path.join(run_folder, \"inferences\")\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(inference_file):\n",
    "                # print(f\"Missing inference file: {rel_path}\")\n",
    "                run_integrity_report[\"missing_files\"].append(rel_path)\n",
    "                continue\n",
    "\n",
    "            # only read the first line to speed up checks\n",
    "            try:\n",
    "                df = pd.read_json(inference_file, lines=True, nrows=1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading inference file: {rel_path}\")\n",
    "                print(e)\n",
    "                run_integrity_report[\"missing_files\"].append(rel_path)\n",
    "                continue\n",
    "\n",
    "            # check for individual results as it is only written out once the evaluation is complete\n",
    "            if \"individual_scores\" not in df.columns:\n",
    "                # print(f\"Missing individual_scores in: {rel_path}\")\n",
    "                run_integrity_report[\"incomplete_results\"].append(rel_path)\n",
    "                continue\n",
    "            complete_task_count += 1\n",
    "\n",
    "    run_integrity_report[\"task_count\"] = task_count\n",
    "    run_integrity_report[\"complete_task_count\"] = complete_task_count\n",
    "    # check status of run\n",
    "    if (\n",
    "        len(run_integrity_report[\"error\"]) == 0\n",
    "        and len(run_integrity_report[\"missing_files\"]) == 0\n",
    "        and len(run_integrity_report[\"incomplete_results\"]) == 0\n",
    "    ):\n",
    "        run_integrity_report[\"status\"] = \"Complete\"\n",
    "        run_integrity_report[\"remarks\"] = \"\"\n",
    "    elif (\n",
    "        len(run_integrity_report[\"missing_files\"]) > 0\n",
    "        or len(run_integrity_report[\"incomplete_results\"]) > 0\n",
    "    ):\n",
    "        missing_count = len(run_integrity_report[\"missing_files\"])\n",
    "        if missing_count > 0:\n",
    "            missing_status = f\"{missing_count} Missing files\"\n",
    "        else:\n",
    "            missing_status = \"\"\n",
    "\n",
    "        incomplete_count = len(run_integrity_report[\"incomplete_results\"])\n",
    "        if incomplete_count > 0:\n",
    "            incomplete_status = f\"{incomplete_count} Incomplete results\"\n",
    "        else:\n",
    "            incomplete_status = \"\"\n",
    "\n",
    "        sep = \" | \" if missing_count > 0 and incomplete_count > 0 else \"\"\n",
    "\n",
    "        run_integrity_report[\"status\"] = \"Incomplete\"\n",
    "        run_integrity_report[\"remarks\"] = f\"{missing_status}{sep}{incomplete_status}\"\n",
    "\n",
    "    return model_name, run, run_integrity_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ec8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = \"\"\n",
    "expected_n_runs = 8\n",
    "\n",
    "pool = Pool(32)\n",
    "model_path_list = glob.glob(os.path.join(main_folder, \"**\", \"**\"))\n",
    "\n",
    "reports = {}\n",
    "starmap_args = []\n",
    "for model_path in model_path_list:\n",
    "    model_integrity_report = {}\n",
    "    model_name = os.path.basename(model_path)\n",
    "\n",
    "    starmap_args.extend(\n",
    "        [(model_name, model_path, run) for run in range(expected_n_runs)]\n",
    "    )\n",
    "\n",
    "results = pool.starmap(run_level_integrity_report, starmap_args)\n",
    "\n",
    "for model_name, run, run_integrity_report in results:\n",
    "    if model_name not in reports:\n",
    "        reports[model_name] = {}\n",
    "\n",
    "    reports[model_name][f\"run_{run}\"] = run_integrity_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a739e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_report_into_df(reports):\n",
    "    outputs = []\n",
    "    for model_name, model_report in reports.items():\n",
    "        for run, run_report in model_report.items():\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"model\": model_name,\n",
    "                    \"run\": run,\n",
    "                    \"git_commit\": run_report.get(\"git_commit\", None),\n",
    "                    \"status\": run_report[\"status\"],\n",
    "                    \"task_count\": f\"{run_report['complete_task_count']}/{run_report['task_count']}\",\n",
    "                    \"remarks\": run_report[\"remarks\"],\n",
    "                    \"is_base_model\": run_report.get(\"is_base_model\", None),\n",
    "                    \"is_reasoning_model\": run_report.get(\"is_reasoning_model\", None),\n",
    "                    \"errors\": \"\\n\".join(run_report[\"error\"]),\n",
    "                    \"missing_files\": \"\\n\".join(run_report[\"missing_files\"]),\n",
    "                    \"incomplete_results\": \"\\n\".join(run_report[\"incomplete_results\"]),\n",
    "                }\n",
    "            )\n",
    "    df = pd.DataFrame(outputs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_report_into_df(reports)\n",
    "color_map = get_color_map(df, \"git_commit\")\n",
    "display(df.style.pipe(styling_fn, color_map=color_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb922da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipynb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
